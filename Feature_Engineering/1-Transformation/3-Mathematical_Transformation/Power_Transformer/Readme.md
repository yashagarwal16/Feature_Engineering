# âš¡ Power Transformation in Machine Learning

## ğŸ“˜ What is Power Transformation?

**Power Transformation** is a technique used to make skewed data more **normal (bell-shaped)** by applying mathematical functions.  
It helps machine learning models that perform better with **normally distributed** features.

---

## ğŸ¯ Why Use Power Transformation?

- To **reduce skewness** in the data
- To **normalize** feature distributions
- To help algorithms like **Linear Regression**, **Logistic Regression**, and **SVM** perform better
- To **stabilize variance**

---

## ğŸ”§ Types of Power Transformations

### ğŸ“Œ 1. Box-Cox Transformation
- Works **only for positive data**
- Automatically finds the best transformation (lambda `Î»`)
- Formula:
```

f(x) = (x^Î» - 1) / Î», if Î» â‰  0
log(x),          if Î» = 0

```

### ğŸ“Œ 2. Yeo-Johnson Transformation
- Works with **positive, zero, and negative values**
- Similar to Box-Cox but more flexible
- Also learns the best Î» to make data close to normal

---

## ğŸ“Š Visual Example

### Original Skewed Data

```

```
 *
 *
**
**
```

---

---

---

```

### After Power Transformation (More Normal)

```

```
  ***
******
```

---

---

---

```
******
  ***
```

````

---

## ğŸ§ª Python Code Example

```python
from sklearn.preprocessing import PowerTransformer

# Using Yeo-Johnson (default)
pt = PowerTransformer(method='yeo-johnson')
transformed_data = pt.fit_transform(data)

# Optional: Box-Cox for positive-only data
pt_boxcox = PowerTransformer(method='box-cox')
transformed_data = pt_boxcox.fit_transform(positive_data)
````

---

## âœ… Summary Table

| Transformation | Supports Negatives | Learns Best Î» | Use When        |
| -------------- | ------------------ | ------------- | --------------- |
| Box-Cox        | âŒ No               | âœ… Yes         | Data > 0        |
| Yeo-Johnson    | âœ… Yes              | âœ… Yes         | Data may be â‰¤ 0 |

---

## ğŸ“Œ When to Use

* Your data is **heavily skewed**
* You want to **normalize** numeric features
* Your model **assumes normal distribution** or performs poorly on raw data

---

## ğŸ“ Use in ML Projects

Power transformation can be a **key part of preprocessing pipelines** to improve:

* Model accuracy
* Training speed
* Feature scaling quality

---

```